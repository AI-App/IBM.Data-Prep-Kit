{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Demo: Data Prep Kit</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Welcome to the demo notebook! Inside, you will find an end-to-end sample data pipeline designed for processing code datasets, beginning with GitHub repositories (.zip files) and culminating in processed data. This notebook provides the following transforms for processing the data. \n",
    "\n",
    "- [Ingest2parquet](#item1)\n",
    "- [Exact Dedup](#item2)\n",
    "- [Doc_ID generation](#item3)\n",
    "- [Fuzzy Dedup](#item4)\n",
    "- [Programming Language Select](#item5)\n",
    "- [Code quality](#item6)\n",
    "- [Filtering](#item7)\n",
    "- [Tokenization](#item8)\n",
    "\n",
    "### Getting started\n",
    "\n",
    "If you want to try this pipeline on your data, you need to download your github repositories, as .zip files. Please refer to steps below for the same. One can also try it on sample data by downloading a few repos of interest.\n",
    "\n",
    "Here's how to download a GitHub repository in ZIP format:\n",
    "\n",
    "1. Go to the desired repository on GitHub.\n",
    "2. Click the \"Code\" button near the top right corner of the repository.\n",
    "3. Click the \"Download ZIP\" button.\n",
    "\n",
    "This will download a ZIP archive of the entire repository to your computer.\n",
    "\n",
    "Follow these steps and download some repositories from github into a folder. Now your data is ready.\n",
    "\n",
    "The folder containing this data would serve as the input to the pipeline. Assign the path of this data folder to the variable `zip_input_folder` in the below cell. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.runtime.ray import RayTransformLauncher\n",
    "from data_processing.utils import ParamsUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# We can set input paths here\n",
    "zip_input_folder = \"input_data\"\n",
    "\n",
    "\n",
    "if not os.path.exists(zip_input_folder):\n",
    "    print (\"NO INPUT DATA\")\n",
    "    print (\"Please set `zip_input_folder` variable to path containing data\")\n",
    "\n",
    "# make sure the paths are correct\n",
    "data_base_path = \"test-data\"\n",
    "\n",
    "parquet_data_output = os.path.join(data_base_path, \"parquet_input\")\n",
    "\n",
    "ededup_out =  os.path.join(data_base_path, \"ededup_out\")\n",
    "\n",
    "doc_id_out =  os.path.join(data_base_path, \"doc_id_out\")\n",
    "fdedup_out = os.path.join(data_base_path, \"fdedup_out\")\n",
    "\n",
    "lang_out =  os.path.join(data_base_path,\"lang_out\")\n",
    "cq_out = os.path.join(data_base_path,\"cq_out\")\n",
    "\n",
    "filter_out = os.path.join(data_base_path ,\"filter_out\")\n",
    "tokensization_out = os.path.join(data_base_path ,\"tokenization_out\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\"> 1. Convert data to parquet using ingest2parquet [<-](#top)<a class=\"anchor\" id=\"item1\"></a>\n",
    "_zip_ to _parquet_ </span>\n",
    "\n",
    "Raw code data files which are in zip format are converted to parquet files, where each row of the parquet file corresponds to a separate code file. Apart from the contents of the code file, every row also contains a unique document id, file URL, name of the repository, source of the data, date of acquisition and license of the repository. For every code file, a language field is also added, which is detected using the filename\n",
    "extensions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this stage input folder contains the zip files, each zip file contains a github repo.\n",
    "\n",
    "input_folder = zip_input_folder\n",
    "output_folder =  parquet_data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:27 INFO - data factory data_ is using local data access: input_folder - input_data output_folder - test-data/parquet_input\n",
      "15:47:27 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:47:27 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.zip']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files is 1 \n",
      "filepath /Users/himapatel/Work/Projects/MCD/OpenSource/Demo/data-prep-kit/tools/ingest2parquet/src/utils/lang_extensions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xf8 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xf8 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xac in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x8c in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xfd in position 10: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xb5 in position 10: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xc4 in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xd8 in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xc4 in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xd8 in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xb0 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xb0 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x98 in position 77: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x98 in position 77: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xe2 in position 114: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xe2 in position 114: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x88 in position 119: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xb8 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xb8 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xe0 in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xe0 in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xdc in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xdc in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xca in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xca in position 7: invalid continuation byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xa0 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xa0 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xf8 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xf8 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xf8 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xb0 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xb0 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x80 in position 7: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x8a in position 70: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xa0 in position 81: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xa0 in position 81: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xa8 in position 85: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0x98 in position 77: invalid start byte\n",
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xc0 in position 7: invalid start byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " skipping data-prep-kit-dev/data-processing-lib/doc/processing-architecture.jpg No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/src/data_processing/__init__.py No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/src/data_processing/test_support/launch/__init__.py No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/daf/input/ds1/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/daf/input/ds1/sample2.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/daf/input/ds2/sample3.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/daf/output/ds1/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/input/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/input_multiple/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/input_multiple/sample2.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/input_multiple/sample3.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/ray/noop/expected/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/ray/noop/expected/subdir/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/ray/noop/input/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/data-processing-lib/ray/test-data/data_processing/ray/noop/input/subdir/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/doc/data-flow.jpg No contents decoded\n",
      " skipping data-prep-kit-dev/doc/data-transformation.jpg No contents decoded\n",
      " skipping data-prep-kit-dev/examples/requirements.txt No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/create_run1.png No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/create_run2.png No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/execution_result.png No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/kfp_ui.png No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/noop_pipeline.png No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/param_list1.png No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/param_list2.png No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/podman_vm_settings.png No contents decoded\n",
      " skipping data-prep-kit-dev/kfp/doc/super_pipeline.png No contents decoded\n",
      " skipping data-prep-kit-dev/tools/ingest2parquet/test-data/expected/application-java.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/tools/ingest2parquet/test-data/expected/https___github.com_00000o1_environments_archive_refs_heads_master.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/tools/ingest2parquet/test-data/input/application-java.zip No contents decoded\n",
      " skipping data-prep-kit-dev/tools/ingest2parquet/test-data/input/https___github.com_00000o1_environments_archive_refs_heads_master.zip No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/code/code_quality/test-data/expected/sample_1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/code/code_quality/test-data/expected/sample_2.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/code/code_quality/test-data/input/sample_1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/code/code_quality/test-data/input/sample_2.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/code/malware/test-data/expected/sample.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/code/malware/test-data/input/sample.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/code/proglang_select/test-data/expected/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/code/proglang_select/test-data/input/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/doc_id/test-data/expected/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/doc_id/test-data/input/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/ededup/images/exactdedup.png No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/ededup/test-data/expected/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/ededup/test-data/input/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/fdedup/images/fuzzy.png No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/fdedup/test-data/expected/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/fdedup/test-data/expected/snapshot/buckets/buckets_collector_0 No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/fdedup/test-data/expected/snapshot/docs/doc_collector_0 No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/fdedup/test-data/expected/snapshot/docs/doc_collector_1 No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/fdedup/test-data/expected/snapshot/minhash/minhash_collector_0 No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/fdedup/test-data/input/sample1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-and-local/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-and/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-datetime-like-local/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-datetime-like/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-default-local/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-default/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-in-local/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-in/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-or-local/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/expected/test-or/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/filter/test-data/input/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/noop/test-data/expected/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/noop/test-data/input/test1.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds01/expected/lang=en/dataset=cybersecurity_v2.0/version=2.3.2/pq03.snappy.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds01/expected/lang=en/pq01.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds01/expected/lang=en/pq02.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds01/input/lang=en/dataset=cybersecurity_v2.0/version=2.3.2/pq03.snappy.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds01/input/lang=en/dataset=empty/dpv08_cc01.snappy.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds01/input/lang=en/dataset=empty/dpv08_cc02.snappy.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds01/input/lang=en/pq01.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds01/input/lang=en/pq02.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds02/expected/df_17m.parquet No contents decoded\n",
      " skipping data-prep-kit-dev/transforms/universal/tokenization/test-data/ds02/input/df_17m.parquet No contents decoded\n",
      "processing stats generated {'total_files_given': 1, 'total_files_processed': 1, 'total_files_failed_to_processed': 0, 'total_no_of_rows': 361, 'total_bytes_in_memory': 1355605, 'failure_details': []}\n",
      "Metadata file stored - response: {'name': 'test-data/parquet_input/metadata.json', 'size': 327}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:28 ERROR - Error -> 'utf-8' codec can't decode byte 0xa7 in position 21: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "import sys\n",
    "import ast\n",
    "from ingest2parquet import ingest2parquet\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "params = {\n",
    "        \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "        \"data_files_to_use\": ast.literal_eval(\"['.zip']\"),\n",
    "        \"detect_programming_lang\": True,\n",
    "        \"snapshot\": \"github\",\n",
    "        \"domain\": \"code\"\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# Launch\n",
    "\n",
    "ingest2parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "##  <span style=\"color: green\">   2. Exact Dedup [<-](#top)<a class=\"anchor\" id=\"item2\"></a> </span>\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58a0e1f6-ff53-40aa-96b1-096ade4bd1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/parquet_input\n",
      "test-data/ededup_out\n"
     ]
    }
   ],
   "source": [
    "## For this stage the input is the folder containing parquet data which is output from the ingest2parquet tool\n",
    "\n",
    "input_folder = parquet_data_output\n",
    "output_folder = ededup_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:48:33 INFO - Running locally\n",
      "15:48:33 INFO - exact dedup params are {'hash_cpu': 0.5, 'num_hashes': 2, 'doc_column': 'contents'}\n",
      "15:48:33 INFO - data factory data_ is using local data access: input_folder - test-data/parquet_input output_folder - test-data/ededup_out\n",
      "15:48:33 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:48:33 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet']\n",
      "15:48:33 INFO - number of workers 3 worker options {'num_cpus': 0.8}\n",
      "15:48:33 INFO - pipeline id pipeline_id; number workers 3\n",
      "15:48:33 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:48:33 INFO - code location None\n",
      "15:48:33 INFO - actor creation delay 0\n",
      "2024-05-14 15:48:37,185\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=53568)\u001b[0m 15:48:39 INFO - orchestrator started at 2024-05-14 15:48:39\n",
      "\u001b[36m(orchestrate pid=53568)\u001b[0m 15:48:39 INFO - Number of files is 1, source profile {'max_file_size': 0.4422798156738281, 'min_file_size': 0.4422798156738281, 'total_file_size': 0.4422798156738281}\n",
      "\u001b[36m(orchestrate pid=53568)\u001b[0m 15:48:39 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 39.97669982910156, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=53568)\u001b[0m 15:48:39 INFO - Number of workers - 3 with {'num_cpus': 0.8} each\n",
      "\u001b[36m(orchestrate pid=53568)\u001b[0m 15:48:39 INFO - Completed 0 files in 7.299582163492839e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=53568)\u001b[0m 15:48:41 INFO - Completed processing in 0.034180982907613115 min\n",
      "\u001b[36m(orchestrate pid=53568)\u001b[0m 15:48:41 INFO - done flushing in 0.002724170684814453 sec\n",
      "15:48:51 INFO - Completed execution in 0.313483452796936 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ededup transform configuration\n",
    "from ededup_transform import EdedupRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "# create launcher\n",
    "ededup_launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n",
    "ededup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4d00-33bb-4d9a-9f34-4d7f3ee0b7bc",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  3. DOC ID generation [<-](#top)<a class=\"anchor\" id=\"item3\"></a> </span>\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6f62394-fbde-495c-bbbb-83161b006bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/ededup_out\n",
      "test-data/doc_id_out\n"
     ]
    }
   ],
   "source": [
    "# Input for this stage is the output of exact dedeup component\n",
    "# output of this component makes it possible for fdedup component to run on data.\n",
    "\n",
    "input_folder = ededup_out\n",
    "output_folder = doc_id_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6daf36d-686c-4e0a-aabf-ce55f999bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:49:54 INFO - Running locally\n",
      "15:49:54 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'hash_column', 'int_column': 'int_id_column'}\n",
      "15:49:54 INFO - data factory data_ is using local data access: input_folder - test-data/ededup_out output_folder - test-data/doc_id_out\n",
      "15:49:54 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:49:54 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet']\n",
      "15:49:54 INFO - number of workers 3 worker options {'num_cpus': 0.8}\n",
      "15:49:54 INFO - pipeline id pipeline_id; number workers 3\n",
      "15:49:54 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:49:54 INFO - code location None\n",
      "15:49:54 INFO - actor creation delay 0\n",
      "2024-05-14 15:49:59,504\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=53696)\u001b[0m 15:50:02 INFO - orchestrator started at 2024-05-14 15:50:02\n",
      "\u001b[36m(orchestrate pid=53696)\u001b[0m 15:50:02 INFO - Number of files is 1, source profile {'max_file_size': 0.4407463073730469, 'min_file_size': 0.4407463073730469, 'total_file_size': 0.4407463073730469}\n",
      "\u001b[36m(orchestrate pid=53696)\u001b[0m 15:50:02 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 39.2768947603181, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=53696)\u001b[0m 15:50:02 INFO - Number of workers - 3 with {'num_cpus': 0.8} each\n",
      "\u001b[36m(orchestrate pid=53696)\u001b[0m 15:50:02 INFO - Completed 0 files in 9.282430013020833e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=53696)\u001b[0m 15:50:04 INFO - Completed processing in 0.038767898082733156 min\n",
      "\u001b[36m(orchestrate pid=53696)\u001b[0m 15:50:04 INFO - done flushing in 0.003125905990600586 sec\n",
      "15:50:14 INFO - Completed execution in 0.3281435489654541 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from doc_id_transform import DocIDRayTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
    "launcher.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## 4. <span style=\"color: green\">  Fuzzy Dedup [<-](#top)<a class=\"anchor\" id=\"item4\"></a> </span>\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/doc_id_out\n",
      "test-data/fdedup_out\n"
     ]
    }
   ],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "input_folder = doc_id_out\n",
    "output_folder = fdedup_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:51:26 INFO - Running locally\n",
      "15:51:26 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'int_id_column', 'cluster_column': 'hash_column', 'bucket_cpu': 0.5, 'mhash_cpu': 0.5, 'doc_cpu': 0.5, 'num_doc_actors': 2, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 2, 'num_permutations': 64, 'threshold': 0.8, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 0.8}}\n",
      "15:51:26 INFO - data factory data_ is using local data access: input_folder - test-data/doc_id_out output_folder - test-data/fdedup_out\n",
      "15:51:26 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:51:26 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet']\n",
      "15:51:26 INFO - number of workers 3 worker options {'num_cpus': 0.8}\n",
      "15:51:26 INFO - pipeline id pipeline_id; number workers 3\n",
      "15:51:26 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:51:26 INFO - code location None\n",
      "15:51:26 INFO - actor creation delay 0\n",
      "2024-05-14 15:51:30,513\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - orchestrator started at 2024-05-14 15:51:33\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - Number of files is 1, source profile {'max_file_size': 0.4658203125, 'min_file_size': 0.4658203125, 'total_file_size': 0.4658203125}\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 39.44537696894258, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - Number of workers - 3 with {'num_cpus': 0.8} each\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - Fuzzy: num buckets 5, bucket length 11\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - Table preprocessing uses 1 readers\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - created 1 table processor actors\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:33 INFO - Completed 0 files in 9.632110595703125e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:44 INFO - Completed processing in 0.17843671639760336 min\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:44 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:45 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:45 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:46 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:46 INFO - created 2 document actors\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:46 INFO - created 1 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:46 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:46 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=53787)\u001b[0m 15:51:46 INFO - processing buckets 0 long, 1695 short\n",
      "\u001b[36m(BucketsHash pid=53787)\u001b[0m 15:51:46 INFO - Done submitting long buckets\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:48 INFO - Done processing buckets in 0.027178847789764406 min\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:48 INFO - creating document snapshots\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=53795)\u001b[0m 15:51:48 INFO - Waiting bucket processing completion\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:50 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:50 INFO - Completed 0 files in 8.519490559895834e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:58 INFO - Completed processing in 0.13367428382237753 min\n",
      "\u001b[36m(orchestrate pid=53772)\u001b[0m 15:51:58 INFO - done flushing in 0.0031740665435791016 sec\n",
      "15:52:08 INFO - Completed execution in 0.6925825993220012 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "fdedup_launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "fdedup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1d761-62a7-4a12-ad23-b2c268ad8ed2",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  5. Programming language annotation [<-](#top)<a class=\"anchor\" id=\"item5\"></a> </span>\n",
    "\n",
    "The raw data may contains many programming languages. Of this, we would wish to retain a prioritised list of selected programming languages. This component takes a file which has new line separated names of languages we need to select. It annotates the data a new column with boolean values. This column can be used by filter component to select the required languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db05e1e-4c62-4367-93ca-b2ddff95e4b4",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8ec4fb6-fa62-45d1-9aa1-596d7182b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_folder = fdedup_out\n",
    "output_folder = lang_out \n",
    "selected_languages_file = \"./test-data/allowed-code-languages.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e7e5a-064f-4dca-a017-4211f7a3e980",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48dbb2a3-a6f4-4a3d-bb2f-8491fd063611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:01:12 INFO - Running locally\n",
      "16:01:12 INFO - data factory proglang_select_ is using local configuration without input/output path\n",
      "16:01:12 INFO - data factory proglang_select_ max_files -1, n_sample -1\n",
      "16:01:12 INFO - data factory proglang_select_ Not using data sets, checkpointing None, max files -1, random samples -1, files to use None\n",
      "16:01:12 INFO - data factory data_ is using local data access: input_folder - test-data/fdedup_out output_folder - test-data/lang_out\n",
      "16:01:12 INFO - data factory data_ max_files -1, n_sample -1\n",
      "16:01:12 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet']\n",
      "16:01:12 INFO - number of workers 1 worker options {'num_cpus': 0.8}\n",
      "16:01:12 INFO - pipeline id pipeline_id; number workers 1\n",
      "16:01:12 INFO - job details {'job category': 'preprocessing', 'job name': 'proglang_select', 'job type': 'ray', 'job id': 'job_id'}\n",
      "16:01:12 INFO - code location None\n",
      "16:01:12 INFO - actor creation delay 0\n",
      "2024-05-14 16:01:16,116\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:18 INFO - orchestrator started at 2024-05-14 16:01:18\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:18 INFO - Number of files is 1, source profile {'max_file_size': 0.44320201873779297, 'min_file_size': 0.44320201873779297, 'total_file_size': 0.44320201873779297}\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:18 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 38.956433868967, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:18 INFO - Number of workers - 1 with {'num_cpus': 0.8} each\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:18 INFO - Getting supported languages from file ./test-data/allowed-code-languages.txt\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:18 INFO - Supported languages ['Java', 'C', 'Go', 'Text', 'Python', 'Markdown']\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:18 INFO - Placed language list into Ray object storage under referenceObjectRef(001b78d7db9a6593ffffffffffffffffffffffff0100000002e1f505)\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:18 INFO - Completed 0 files in 7.2161356608072914e-06 min. Waiting for completion\n",
      "\u001b[36m(TransformTableProcessorRay pid=54103)\u001b[0m 16:01:20 INFO - Loading languages to include from Ray storage under reference ObjectRef(001b78d7db9a6593ffffffffffffffffffffffff0100000002e1f505)\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:20 INFO - Completed processing in 0.028798997402191162 min\n",
      "\u001b[36m(orchestrate pid=54089)\u001b[0m 16:01:20 INFO - done flushing in 0.0009629726409912109 sec\n",
      "16:01:30 INFO - Completed execution in 0.30742111603418987 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from proglang_select_transform import (\n",
    "    ProgLangSelectRayConfiguration,\n",
    "    lang_allowed_langs_file_key,\n",
    "    lang_lang_column_key,\n",
    "    lang_output_column_key,\n",
    ")\n",
    "\n",
    "# create parameters\n",
    "language_column_name = \"programming_language\"\n",
    "annotated_column_name = \"lang_selected\"\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "langselect_config = {\n",
    "    lang_allowed_langs_file_key: selected_languages_file,\n",
    "    lang_lang_column_key: language_column_name,\n",
    "    lang_output_column_key: annotated_column_name,\n",
    "}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 1,\n",
    "    # language selection specific parameters\n",
    "    **langselect_config,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(ProgLangSelectRayConfiguration())\n",
    "launcher.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cbb7-3046-44c0-827d-d102d3ff7cb8",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  6. Code Quality [<-](#top)<a class=\"anchor\" id=\"item6\"></a> </span>\n",
    "\n",
    "We experiment with various code quality metrics but finally retain\n",
    "the four code quality metrics used by (Li et al., 2023) to balance the tradeoff between\n",
    "code quality versus data volume. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e985668-848b-4633-b0d8-9fe70ada0c91",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f080011-c9fe-430e-9ecc-f2220d2c8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/lang_out\n",
      "test-data/cq_out\n"
     ]
    }
   ],
   "source": [
    "input_folder = lang_out\n",
    "output_folder = cq_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02982c5-f398-4a1a-a9fe-42d7ae748c7c",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29319fb9-b0d8-4f86-9bc5-b92960ad8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:02:42 INFO - Running locally\n",
      "16:02:42 INFO - data factory data_ is using local data access: input_folder - test-data/lang_out output_folder - test-data/cq_out\n",
      "16:02:42 INFO - data factory data_ max_files -1, n_sample -1\n",
      "16:02:42 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet']\n",
      "16:02:42 INFO - number of workers 3 worker options {'num_cpus': 0.8}\n",
      "16:02:42 INFO - pipeline id pipeline_id; number workers 3\n",
      "16:02:42 INFO - job details {'job category': 'preprocessing', 'job name': 'code_quality', 'job type': 'ray', 'job id': 'job_id'}\n",
      "16:02:42 INFO - code location None\n",
      "16:02:42 INFO - actor creation delay 0\n",
      "2024-05-14 16:02:46,129\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=54160)\u001b[0m None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\u001b[36m(orchestrate pid=54160)\u001b[0m 16:02:49 INFO - orchestrator started at 2024-05-14 16:02:49\n",
      "\u001b[36m(orchestrate pid=54160)\u001b[0m 16:02:49 INFO - Number of files is 1, source profile {'max_file_size': 0.4435100555419922, 'min_file_size': 0.4435100555419922, 'total_file_size': 0.4435100555419922}\n",
      "\u001b[36m(orchestrate pid=54160)\u001b[0m 16:02:49 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 39.04623680189252, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=54160)\u001b[0m 16:02:49 INFO - Number of workers - 3 with {'num_cpus': 0.8} each\n",
      "\u001b[36m(orchestrate pid=54160)\u001b[0m 16:02:49 INFO - Completed 0 files in 7.502237955729166e-06 min. Waiting for completion\n",
      "\u001b[36m(TransformTableProcessorRay pid=54172)\u001b[0m /Users/himapatel/Work/Projects/MCD/OpenSource/Demo/data-prep-kit/examples/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[36m(TransformTableProcessorRay pid=54172)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(TransformTableProcessorRay pid=54174)\u001b[0m Token indices sequence length is longer than the specified maximum sequence length for this model (1676 > 1024). Running this sequence through the model will result in indexing errors\n",
      "\u001b[36m(orchestrate pid=54160)\u001b[0m 16:02:53 INFO - Completed processing in 0.07286310195922852 min\n",
      "\u001b[36m(orchestrate pid=54160)\u001b[0m 16:02:53 INFO - done flushing in 0.0032660961151123047 sec\n",
      "16:03:03 INFO - Completed execution in 0.3572404503822327 min, execution result 0\n",
      "\u001b[36m(TransformTableProcessorRay pid=54173)\u001b[0m None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(TransformTableProcessorRay pid=54174)\u001b[0m /Users/himapatel/Work/Projects/MCD/OpenSource/Demo/data-prep-kit/examples/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TransformTableProcessorRay pid=54174)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from code_quality_transform import CodeQualityRayTransformConfiguration\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "language_column_name = \"programming_language\"\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_creation_delay\": 0,\n",
    "    # code quality configuration\n",
    "    \"cq_contents_column_name\": \"contents\",\n",
    "    \"cq_language_column_name\": language_column_name,\n",
    "}\n",
    "\n",
    "\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(CodeQualityRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec9839-13b4-4d32-89c5-38f59c5a89f0",
   "metadata": {},
   "source": [
    "## 7. <span style=\"color: green\">   Filtering [<-](#top)<a class=\"anchor\" id=\"item7\"></a> </span>\n",
    "\n",
    "Filter out documents that do not meet the quality threshold for each annotation. The thresholds are computed based on a distributional\n",
    "analysis as well as manual inspection of samples maintaining the balance between data quality and data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c54d69-8aee-4f0f-b74c-35dc0609270f",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7991811-b19e-43b5-89ac-b24060c0ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = cq_out\n",
    "output_folder = filter_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460e05c-aeee-4b53-9dd5-8dfa1afc0ece",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61dea2b0-0e54-4912-8620-886e2b8420ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:04:48 INFO - Running locally\n",
      "16:04:48 INFO - data factory data_ is using local data access: input_folder - test-data/cq_out output_folder - test-data/filter_out\n",
      "16:04:48 INFO - data factory data_ max_files -1, n_sample -1\n",
      "16:04:48 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet']\n",
      "16:04:48 INFO - number of workers 5 worker options {'num_cpus': 0.8}\n",
      "16:04:48 INFO - pipeline id pipeline_id; number workers 5\n",
      "16:04:48 INFO - job details {'job category': 'preprocessing', 'job name': 'filter', 'job type': 'ray', 'job id': 'job_id'}\n",
      "16:04:48 INFO - code location None\n",
      "16:04:48 INFO - actor creation delay 0\n",
      "2024-05-14 16:04:51,445\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=54239)\u001b[0m 16:04:54 INFO - orchestrator started at 2024-05-14 16:04:54\n",
      "\u001b[36m(orchestrate pid=54239)\u001b[0m 16:04:54 INFO - Number of files is 1, source profile {'max_file_size': 0.4591951370239258, 'min_file_size': 0.4591951370239258, 'total_file_size': 0.4591951370239258}\n",
      "\u001b[36m(orchestrate pid=54239)\u001b[0m 16:04:54 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 39.04187660291791, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=54239)\u001b[0m 16:04:54 INFO - Number of workers - 5 with {'num_cpus': 0.8} each\n",
      "\u001b[36m(orchestrate pid=54239)\u001b[0m 16:04:54 INFO - Completed 0 files in 7.3552131652832035e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=54239)\u001b[0m 16:04:56 INFO - Completed processing in 0.03446693817774455 min\n",
      "\u001b[36m(orchestrate pid=54239)\u001b[0m 16:04:56 INFO - done flushing in 0.0033478736877441406 sec\n",
      "16:05:06 INFO - Completed execution in 0.29624868631362916 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from data_processing.data_access import DataAccessLocal\n",
    "from filter_transform import (\n",
    "    FilterRayTransformConfiguration,\n",
    "    filter_columns_to_drop_cli_param,\n",
    "    filter_criteria_cli_param,\n",
    "    filter_logical_operator_cli_param,\n",
    ")\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "# This is just an example criteria to filter\n",
    "filter_criteria = [\n",
    "    \"total_num_lines > 10 AND total_num_lines < 90\",\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "filter_columns_to_drop = [\"lang_selected\", \"hash_column\"]\n",
    "\n",
    "filter_params = {\n",
    "    filter_criteria_cli_param: filter_criteria,\n",
    "    filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
    "    filter_logical_operator_cli_param: filter_logical_operator,\n",
    "}\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "launcher_params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(launcher_params | filter_params)\n",
    "    # Create the longer to launch with the blocklist transform.\n",
    "launcher =RayTransformLauncher(FilterRayTransformConfiguration())\n",
    "    # Launch the ray actor(s) to process the input\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## 8. <span style=\"color: green\">  Tokenization [<-](#top)<a class=\"anchor\" id=\"item8\"></a> </span>\n",
    "\n",
    "The data tokenization transform maps a (non-empty) input table to an output table using a pre-trained tokenizer. The input table must contain at least two columns, by default named document_id and contents. The tokenization transform utilizes the pre-trained tokenizer to tokenize each row (assuming a document) in the input table to each row in the output folder.\n",
    "\n",
    "A pre-trained tokenizer must be specified through the --tkn_tokenizer parameter, which can be the name of a ready-for-download tokenizer from HuggingFace such as hf-internal-testing/llama-tokenizer, bigcode/starcoder or any others that can loaded by the Huggingface AutoTokenizer library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = filter_out\n",
    "output_folder = tokensization_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:07:10 INFO - Running locally\n",
      "16:07:10 INFO - data factory data_ is using local data access: input_folder - test-data/filter_out output_folder - test-data/tokenization_out\n",
      "16:07:10 INFO - data factory data_ max_files -1, n_sample -1\n",
      "16:07:10 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet']\n",
      "16:07:10 INFO - number of workers 5 worker options {'num_cpus': 0.8}\n",
      "16:07:10 INFO - pipeline id pipeline_id; number workers 5\n",
      "16:07:10 INFO - job details {'job category': 'preprocessing', 'job name': 'Tokenization', 'job type': 'ray', 'job id': 'job_id'}\n",
      "16:07:10 INFO - code location None\n",
      "16:07:10 INFO - actor creation delay 0\n",
      "2024-05-14 16:07:14,722\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from tokenization_transform import TokenizationRayConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 5,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TokenizationRayConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029fecc-fcd5-47c9-afbe-c6a3a9daa558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
