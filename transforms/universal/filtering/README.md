# Filter Design

## Filters Used for Code
Code quality filter collects different stats (like `line_max`, `line_min`, `alpha_frc`, etc) for each row in a parquet and outputs multiple columns with these stats.  In the sift/filtering pipeline, filtering is applied based on the values of these stats.  A complete schema of filter annotations is given below:

| Filter      |  Columns   |
|-------------|------------|
| HAP         | document_id,keep,referenced_bad_word |
| Fuzzy Dedup | document_id, fuzzy_dedup_keep, similar_to |
| License Filtering | document_id, license, license_status |
| Malware Filtering | document_id, virus_detection |
| PII Detection and Redaction | document_id , ft_pii |
| Code Quality Filtering | document_id, contents, language, size, line_mean, line_max, avg_longest_lines, alpha_frac, char_token_ratio, autogenerated, config_or_test, has_no_keywords, has_few_assignments, filter_xml, filter_html, filter_json, filter_yaml, filter_openshift_k8, filter_ansible_yaml, filter_cobol, filter_abap, filter_size, code_filters_keep |
| Exact Dedup | - |
| Programming Language | - |

## Filters Used for Language
Language uses four types of filters: document quality filters, HAP filters, PII filters and block listing filters. A brief description of each filter type is given below.

### Document Quality Filters:
Similar to the code quality filters, document quality filters retain the documents that have specific values for various stats. The first filter below drops the documents with a perplexity score higher than `188.54`. The second filter drops the document based on a more complex condition that uses the values of 8 different statistics (`docq_total_words`, `docq_mean_word_len`, `docq_symbol_to_word_ratio`, `docq_bullet_point_ratio`, `docq_ellipsis_line_ratio`, `docq_alphabet_word_ratio`, `docq_contain_common_en_words`, and `ibmkenlm_docq_perplex_score`). 

Filter 1:
```
    if lang == 'en':
        cond = """
            (ibmkenlm_docq_perplex_score <= 188.54) 
        """
```

Filter 2:
```
    if lang == 'en':
        cond = """
            (docq_total_words >= 50) and (docq_total_words <=100000) and (docq_mean_word_len >= 3) and (docq_mean_word_len <= 10) and (docq_symbol_to_word_ratio <= 0.1) and (docq_bullet_point_ratio < 0.9) and (docq_ellipsis_line_ratio <= 0.3) and (docq_alphabet_word_ratio >= 0.8) and (docq_contain_common_en_words = true) and (ibmkenlm_docq_perplex_score < 500) 
        """
    elif lang == 'ja':
        ...
```

### HAP Filter:
The HAP filter uses `hap_score`, a per-sentence annotation. The sentence records are grouped by document, and the documents are filtered based on a  condition in this format:
```
    "commoncrawl":
    {
        "en": [[0.6, 0.5], [0.85, 0]],
    },
```
The condition above refers to the data set `commoncrawl`, for documents in English language. It filters out all the documents that either have more than `0.5%` of the sentences with a `hap_score` greater than `0.6` or have at least one sentence with a `hap_score` greater than `0.85`.  

### PII Filter:
The PII filter takes as input a PII Annotation (in JSON format) and applies to it a user-defined function, resulting in a new column:
```
def check_pi(s):
    s = json.loads(s)
    types=s.get("counts",{}).get("types",{})
    if types and \
       (types.get('BankAccountNumber', 0) > 0) or \
       (types.get('EmailAddress', 0) > 1) or \
       (types.get('PhoneNumber', 0) > 2) or \
       (types.get('IPAddress', 0) > 0) or \
       (types.get('Location', 0) > 1): # or \
       # (types.get('Date',0)>10) or \
       # (types.get('Person',0)>12):
        return f"{types}"
    return ""
```
All the rows for which the PII column is not "" are filtered out.

### Blocklist Filter:
The blocklist filter takes as input a list of blocked domains and filters out the documents originating from those domains. The filter first creates an annotation column that is empty, or contains the name of a blocked domain, and then all the rows for which the blocklist column is not empty are filtered out.

### RedPajamaV2 Filters
A number of open source data sets (initially Dolma, but now this convention has also been adopted by Red Pajama V2), define the quality signals in a list of lists format, with each list containing three values: the beginning/end of the text (in the current document) to which the signal applies and the signal value for that text. For example, the blocklisting for a document of length 1001 will be given in the format:
```
[[0, 1000, 23]] - this is a bad document, 23 corresponds to a bad category e.g. gambling, aggressive, adult, etc.
[[0, 1000, null]] - this is a good document because bad category is null
```
This allows providing complex signals for a single document in one row.  For example, hap_score now for a document with three sentences will be provided like this:
```
[[0, 111, 0.002], [112, 834, 0.003], [835, 1000, 0.99]]
```
where the first sentence starts in position `0`, the second sentence starts in position `112`, and the third sentence starts in position `835`. This format allows to give per-sentence signals for an entire document in one row, as opposed to allocating a separate row for each sentence in the document. 

Language has two new filters that use this (Dolma / RedPajamasV2) format for the quality signals:

New Blocklisting Filter:
```
quality_signals.rps_doc_ut1_blacklist[-1][-1] is None
```
where quality_signals.rps_doc_ut1_blacklist is a list of lists, as described above.

New Document Quality Filter:
```
language_score > 0.5 and perplexity < 520 and quality_signals.rps_doc_ml_wikiref_score[-1][-1] >= 0.25
```
where quality_signals.rps_doc_ml_wikiref_score is again in the list of lists format, described above

## Filter Design Requirements
A filtering design requirement is to give the users the ability to define new filtering conditions, as well as different threshold values for each filter parameter. The goal is to build a general purpose filter that can be used on the results of many other annotating transforms (code quality, text quality, pii, blocklisting, etc).

### Using SQL as a Query Language
A natural candidate for this task is SQL, a language designed to query data for relational databases. Providing a filtering API that takes a SQL query as input would give the users the same level of flexibility as querying a relational database. In order for the SQL to work and be applicable for filtering, we need to have the data in a table, where the columns we want to filter on have basic types (`string`, `integer`, `float`). The tables on which we do filtering must be, therefore, already be in a format that is compatible with the SQL standard. (TODO: in addition to the string and numeric types, we might also be able to provide support for JSON fields, or list fields).

### Current Filter Compatibility with SQL
The code filters collect different stats (like `line_max`, `line_min`, `alpha_frc` & etc) for each row in a parquet and output multiple columns with these stats.  In sift/filtering pipeline, filtering is applied based on these stats value. By design, code filters are already compatible with an SQL query interface.

From the language filters, text quality and block listing are compatible with SQL. Further processing is required on the PII filter to transform a relatively complex JSON structure into one or several columns that are SQL-readable. The HAP filter cannot be easily adapted to SQL, because it is currently relying on grouping `hap_score` for sentences into document stats, and then calculate two additional conditions (one or several conditions to drop the document if a certain percentage of sentences have a hap_score larger than a certain threshold) on top of this aggregation. 

### Filtering Using SQL
| Code Filter      |  Sample SQL   |
|-------------|------------|
| HAP         | SELECT * from pyarrow_table WHERE keep = true |
| Fuzzy Dedup | SELECT * from pyarrow_table WHERE fuzzy_dedup_keep = true |
| License Filtering | SELECT * from pyarrow_table WHERE license IN ('Apache', 'MIT') |
| Malware Filtering | SELECT * from pyarrow_table WHERE virus_detection = false |
| PII Detection and Redaction | SELECT * from pyarrow_table WHERE ft_pii.person_count < 2 AND ft_pii.bank_account_count < 1 |
| Code Quality Filtering | SELECT * from pyarrow_table WHERE language = 'en' AND size > 1000 |
| Exact Dedup | - |
| Programming Language | - |

| Language Filter      |  Sample SQL   |
|-------------|------------|
| Document Quality Filtering | SELECT * from pyarrow_table WHERE language_score > 0.5 AND perplexity <= 520 AND wikiref_score >= 0.25 |
| PII Filtering | SELECT * from pyarrow_table WHERE ft_pii.counts.types.BankAccountNumber < 1 AND ft_pii.counts.types.EmailAddress < 3 |
| Block List Filtering | SELECT * from pyarrow_table WHERE url_blocklisting_refinedweb ='' |
| License Filtering | SELECT * from pyarrow_table WHERE license IN ('Apache', 'MIT') |

## Implement Filtering Using SQL Prepared Statements with Named Parameters
Our tables are in `pyarrow.Table` format. We can implement a SQL query API on top of these table using the `duckdb` package.  

An important security aspect that needs to be taken into account in filtering using SQL as query language is [prevention of SQL injection attacks](https://realpython.com/prevent-python-sql-injection/). To solve this problem, DuckDB provides the capability to use [prepared statements with named parameters](https://duckdb.org/docs/api/python/dbapi).

This requires any filter operation to take two parameters:  
 1. The prepared SQL statement
 2. A dictionary that provides the mapping of the named parameters to their values

For example, assume we want to retrieve from a `pyarrow` table the list of documents for which the language score is greater than `0.5` and the perplexity score is less than `520.0`. We can use the filtering code below, where `prepared_sql_statement` and `named_parameters_mapping` can be passed as input parameters:
```
import duckdb
import pyarrow as pa

# build the prepared sql statement with named parameters; these can be passed as input parameters
prepared_sql_statement = "SELECT * FROM input_table WHERE lang_score >= $lang_score AND perplexity <= $perplexity_score"
named_parameters_mapping = {"lang_score": 0.5, "perplexity_score": 520.0}

# build a sample pyarrow table
names = ["doc_id", "lang_score", "perplexity"]
doc_ids = pa.array(["doc-1", "doc-2", "doc-3", "doc-4"])
lang_scores = pa.array([0.7, 0.32, 0.99, 0.85])
perplexity_scores = pa.array([100.0, 600.0, 111.1, 993.3])
input_table = pa.Table.from_arrays([doc_ids, lang_scores, perplexity_scores], names = names)

# execute prepared SQL query 
results = duckdb.execute(prepared_sql_statement, named_parameters_mapping).fetchall()
print(results)
# this will print: [('doc-1', 0.7, 100.0), ('doc-3', 0.99, 111.1)]
# the results, as expected, will drop doc-2, because its language score is less than 0.5, and doc-4, because its perplexity is greater than 520
```
Aside from covering a well-known security loophole, this solution has two other benefits. First, it provides a very clear and readable way to define the SQL query and its parameters. Second, these inputs can be used to calculate filtering statistics that indicate how many documents were dropped because of each filtering named parameter.

### Additional Filtering Functionality
1. Run Preview Filtering on a Small Random Subset of the Data Set Files  
   This can be achieved by selecting the small random subset of files in the Filter Runtime.
2. Gather execution statistics:
   * number of files processed / failed (and the reason why they failed) / empty
   * number of input / output bytes / documents
   * number of documents filtered because of each filtering condition
3. Checkpoint and re-run filtering for the files that failed to process for specific reasons (such as failing to download due to timeout, or overload, from the COS S3 bucket)
